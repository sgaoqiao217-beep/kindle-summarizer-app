import io
import os
import re
import json
import tempfile
import unicodedata
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional

import streamlit as st
from PIL import Image
from dotenv import load_dotenv

try:
    from google import genai
    from google.genai import types
except Exception:
    genai = None
    types = None

try:
    from googleapiclient.discovery import build
    from googleapiclient.http import MediaIoBaseDownload
except Exception:
    build = None
    MediaIoBaseDownload = None

try:
    from google_auth_oauthlib.flow import InstalledAppFlow
except Exception:
    InstalledAppFlow = None

try:
    from google_api_utils import (
        get_credentials as get_google_credentials,
        create_google_doc as create_google_doc_external,
    )
except Exception:
    get_google_credentials = None
    create_google_doc_external = None
# --- Fallback for google_api_utils ãŒç„¡ã„ç’°å¢ƒ ---
if get_google_credentials is None or create_google_doc_external is None:
    from google.oauth2 import service_account
    # googleapiclient.build ã¯ app.py å…ˆé ­ã§ try-import æ¸ˆã¿ï¼ˆbuild ãŒ None ã®å¯èƒ½æ€§ã‚ã‚Šï¼‰

    _SERVICE_ACCOUNT_SCOPES = [
        "https://www.googleapis.com/auth/documents",
        "https://www.googleapis.com/auth/drive",
    ]

    _USER_OAUTH_SCOPES = [
        "https://www.googleapis.com/auth/drive.file",
        "https://www.googleapis.com/auth/documents",
    ]

    def _load_json_secret(raw):
        return json.loads(raw) if isinstance(raw, str) else raw

    def _get_service_account_credentials():
        raw = st.secrets["GOOGLE_CREDENTIALS"]  # secrets.toml ã« JSON å…¨æ–‡ã‚’å…¥ã‚Œã‚‹é‹ç”¨
        info = _load_json_secret(raw)
        return service_account.Credentials.from_service_account_info(info, scopes=_SERVICE_ACCOUNT_SCOPES)

    def _get_user_oauth_creds():
        if InstalledAppFlow is None:
            raise ImportError("google-auth-oauthlib ãŒå¿…è¦ã§ã™ã€‚`pip install google-auth-oauthlib` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        if "google_oauth" not in st.secrets:
            raise KeyError("st.secrets['google_oauth'] ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        section = st.secrets["google_oauth"]
        try:
            client_json = section["client_json"]
        except Exception as exc:
            raise KeyError("st.secrets['google_oauth']['client_json'] ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“") from exc
        config = _load_json_secret(client_json)
        flow = InstalledAppFlow.from_client_config(config, _USER_OAUTH_SCOPES)
        creds = flow.run_local_server(port=0)
        st.write("Using user OAuth credentials (files count toward your Drive quota).")
        return creds

    def get_google_credentials(use_user_oauth: bool | None = None):
        """Secretsã«å¿œã˜ã¦ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ or OAuth ã‚’é¸æŠ"""
        if use_user_oauth is None:
            use_user_oauth = "google_oauth" in st.secrets
        if use_user_oauth:
            return _get_user_oauth_creds()
        return _get_service_account_credentials()

    def _ensure_folder_path(drive_service, parts):
        """['OCRçµæœ','æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«'] ã®ã‚ˆã†ãªãƒ‘ã‚¹ã‚’Driveä¸Šã«ä½œæˆã—ã¦è¦ªIDã‚’è¿”ã™"""
        parent_id = None
        for name in parts:
            q = "mimeType='application/vnd.google-apps.folder' and name='%s'" % name.replace("'", r"\'")
            if parent_id:
                q += f" and '{parent_id}' in parents"
            res = drive_service.files().list(q=q, fields="files(id,name)", pageSize=1).execute()
            items = res.get("files", [])
            if items:
                parent_id = items[0]["id"]
            else:
                meta = {"name": name, "mimeType": "application/vnd.google-apps.folder"}
                if parent_id:
                    meta["parents"] = [parent_id]
                created = drive_service.files().create(body=meta, fields="id").execute()
                parent_id = created["id"]
        return parent_id

    def create_google_doc_external(book_title: str, chapter_title: str, text: str, creds, root_name: str = "OCRçµæœ"):
        if build is None:
            raise ImportError("googleapiclient ãŒå¿…è¦ã§ã™ã€‚`pip install google-api-python-client` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        docs = build("docs", "v1", credentials=creds)
        drive = build("drive", "v3", credentials=creds)

        # Driveã®ä¿å­˜å…ˆã‚’ç”¨æ„: OCRçµæœ/æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«
        folder_id = _ensure_folder_path(drive, [root_name, book_title])

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ
        doc_title = f"{book_title}ï¼ˆ{chapter_title}ï¼‰"
        doc = docs.documents().create(body={"title": doc_title}).execute()
        doc_id = doc["documentId"]

        # 1è¡Œç›®ã‚’è¦‹å‡ºã—2ã«ã—ã¦æœ¬æ–‡ã‚’æŒ¿å…¥ï¼ˆé‡è¤‡è¦‹å‡ºã—ã‚’é¿ã‘ã‚‹ï¼‰
        heading = (chapter_title or "ç„¡é¡Œ").strip()
        body_text = (text or "").lstrip()
        first = (body_text.splitlines() or [""])[0].strip()
        if first == f"## {heading}":
            body_text = "\n".join(body_text.splitlines()[1:]).lstrip()

        content = f"{heading}\n\n{body_text}".rstrip() + "\n"
        requests = [
            {"insertText": {"location": {"index": 1}, "text": content}},
            {
                "updateParagraphStyle": {
                    "range": {"startIndex": 1, "endIndex": 1 + len(heading) + 1},
                    "paragraphStyle": {"namedStyleType": "HEADING_2"},
                    "fields": "namedStyleType",
                }
            },
        ]
        docs.documents().batchUpdate(documentId=doc_id, body={"requests": requests}).execute()

        # ä½œæˆç›´å¾Œã¯ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–ç›´ä¸‹ãªã®ã§ã€ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã¸ç§»å‹•
        meta = drive.files().get(fileId=doc_id, fields="parents").execute()
        prev = ",".join(meta.get("parents", []))
        drive.files().update(
            fileId=doc_id, addParents=folder_id, removeParents=prev, fields="id, parents"
        ).execute()

        return doc_id
# --- Fallback ã“ã“ã¾ã§ ---

# Optional: æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒã‚ã‚Œã°ä½¿ã†ï¼ˆç„¡ã‘ã‚Œã°ImportErrorã‚’æ¡ã‚Šã¤ã¶ã—ã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
try:
    from sort_image_file import normalize_filenames as normalize_filenames_external
except Exception:
    normalize_filenames_external = None

try:
    from ocr_utils import extract_info_type1 as extract_info_external
    from ocr_utils import _vision_client_from_secrets 
    # â†‘ ãƒ¦ãƒ¼ã‚¶ç’°å¢ƒã®é–¢æ•°åã«åˆã‚ã›ã¦ãŠå¥½ã¿ã§
except Exception:
    extract_info_external = None


from google.oauth2 import service_account

load_dotenv()

# å¥ç‚¹ã‚„æ‹¬å¼§ã§æ®µè½/æ–‡æœ«ã‚’åˆ¤æ–­ã™ã‚‹ãŸã‚ã®å®‰å…¨ãªçµ‚ç«¯è¨˜å·ã‚»ãƒƒãƒˆ
_JP_SENT_END = "ã€‚ï¼ï¼ï¼Ÿ!?" + "ã€ã€ï¼‰ã€‘ï¼½ã€‹ã€‰"
# æ–‡é ­ã«æ¥ã‚„ã™ã„å§‹ã¾ã‚Šè¨˜å·ï¼ˆæ¬¡è¡ŒãŒã“ã‚Œã§å§‹ã¾ã‚‹ãªã‚‰æ”¹è¡Œã‚’æ®‹ã™ï¼‰
_JP_SENT_BEGIN = "ã€Œã€ï¼ˆã€ï¼»ã€Šã€ˆ"

# =========================================================
# â˜… è¿½åŠ ï¼šè¦ç´„å‰ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆè¦‹å‡ºã—ä¿®æ­£ãƒ»ãƒ¬ãƒ³ã‚¸è¡Œå‰Šé™¤ãƒ»æ–‡ä¸­æ”¹è¡Œã®è§£æ¶ˆï¼‰
# =========================================================
def clean_presummary_text(text: str) -> str:
    """
    ç›®çš„:
      1) è¦‹å‡ºã— 'Part 01.doc' â†’ 'Part01' ã«ç½®æ›ï¼ˆ.docå‰Šé™¤ & åŠè§’ã‚¹ãƒšãƒ¼ã‚¹é™¤å»ã€ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ç¶­æŒï¼‰
      2) 'Part 01 (1â€“9,278)' ã®ã‚ˆã†ãªãƒšãƒ¼ã‚¸ãƒ¬ãƒ³ã‚¸è¡Œã®å®Œå…¨å‰Šé™¤
      3) æ–‡ç« é€”ä¸­ã®ä¸è¦æ”¹è¡Œã‚’é™¤å»ï¼ˆæ®µè½ã®ç©ºè¡Œã¯ä¿æŒï¼‰
    """
    if not isinstance(text, str):
        return text

    # æ”¹è¡Œçµ±ä¸€
    t = text.replace("\r\n", "\n").replace("\r", "\n")

    # è¡Œã”ã¨ã«å‰å‡¦ç†ï¼ˆ1,2ã‚’é©ç”¨ï¼‰
    lines = t.splitlines()
    cleaned_lines = []
    for ln in lines:
        # 2) 'Part 01 (1â€“9,278)' ã‚’å‰Šé™¤ï¼ˆãƒã‚¤ãƒ•ãƒ³/ã‚¨ãƒ³ãƒ€ãƒƒã‚·ãƒ¥ãƒ»ã‚«ãƒ³ãƒæ¡åŒºåˆ‡ã‚Šã«å¯¾å¿œï¼‰
        if re.fullmatch(
            r"\s*Part\s*\d+\s*\(\s*\d{1,3}(?:,\d{3})*\s*[â€“-]\s*\d{1,3}(?:,\d{3})*\s*\)\s*\.*\s*",
            ln
        ):
            continue

        # 1) 'Part 01.doc' â†’ 'Part01'
        m = re.fullmatch(r"\s*Part\s*([0-9]{1,3})\s*\.?\s*[dD][oO][cC]\s*", ln)
        if m:
            num = m.group(1).zfill(2)
            cleaned_lines.append(f"Part{num}")
            continue

        cleaned_lines.append(ln)

    t = "\n".join(cleaned_lines).strip("\n")

    # 3) æ®µè½å†…ã®â€œæ–‡ä¸­æ”¹è¡Œâ€ã‚’å‰Šé™¤ã—ã€ç©ºè¡Œã§ã®æ®µè½åŒºåˆ‡ã‚Šã¯ä¿æŒ
    paragraphs = re.split(r"\n{2,}", t)

    def join_soft_wraps(p: str) -> str:
        lines = p.split("\n")
        if len(lines) == 1:
            return lines[0].strip()

        buf = []
        cur = lines[0].strip()

        for nxt in lines[1:]:
            nxt_stripped = nxt.strip()
            if not nxt_stripped:
                # æ®µè½å†…ã«ç©ºè¡ŒãŒç´›ã‚Œã¦ã„ãŸã‚‰æ”¹è¡Œã‚’ç¶­æŒ
                buf.append(cur)
                cur = ""
                continue

            # ç›´å‰ãŒæ–‡æœ«è¨˜å· / æ¬¡è¡Œé ­ãŒé–‹ãè¨˜å· â†’ æ”¹è¡Œç¶­æŒ
            keep_newline = (
                len(cur) > 0 and cur[-1] in _JP_SENT_END
            ) or (
                len(nxt_stripped) > 0 and nxt_stripped[0] in _JP_SENT_BEGIN
            )

            if keep_newline:
                buf.append(cur)
                cur = nxt_stripped
            else:
                # æ–‡é€”ä¸­ã®æ”¹è¡Œã¯çµåˆï¼ˆè‹±æ•°ã©ã†ã—ã®ã¿åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŒ¿å…¥ï¼‰
                if (cur and nxt_stripped and cur[-1].isascii() and nxt_stripped[0].isascii()):
                    cur = cur + " " + nxt_stripped
                else:
                    cur = cur + nxt_stripped

        if cur:
            buf.append(cur)

        return "\n".join(buf)

    joined = [join_soft_wraps(p) for p in paragraphs if p.strip()]
    return "\n\n".join(joined)

format_prompt = """
You are given raw text from a Japanese book (a novel or story).
Your task is ONLY to format the text into a clean book-like structure.

Rules:
1. Insert one line break between paragraphs.
2. At the beginning of each paragraph, insert one full-width space character (å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹1æ–‡å­—åˆ†).
3. Remove unnecessary line breaks inside sentences.
4. If there are unnatural word breaks or typos caused by OCR, fix them using context.
5. Do NOT summarize or shorten the content. Keep all original content.

æœ€çµ‚å‡ºåŠ›ã¯æœ¬æ–‡ã®æ•´å½¢å¾Œãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
ä½™è¨ˆãªèª¬æ˜ã‚„æ³¨é‡ˆã¯ä¸€åˆ‡ä¸è¦ã§ã™ã€‚
"""

summary_prompt_template = """
You are given a chapter from a Japanese novel or story (already formatted).
Your task is to summarize it into a polished, story-like text.

Use the following chapter title exactly as provided:
{chapter_title}

**Output structure (must follow exactly):**

1. First line: the chapter title as an H2 heading.
   * If Google Docs: apply **HEADING_2** style to the title line.
2. One blank line.
3. The summary body text (Japanese), 3000â€“4000 characters, formatted per the rules below.

**Critical constraints (do not violate):**

* Do **not** stop after writing the heading. Writing only the heading is invalid.
* Always include the full summary body after the blank line.
* If the input lacks a clear chapter title, infer a concise title from the content (e.g., main topic or scene) and still output it as H2.

**Rules for the summary body:**

1. Length: **3000â€“4000 Japanese characters** (strict).
2. Include all key events, characters, and emotional flow.
3. Ensure the writing is natural, grammatically correct, and coherent.
4. Preserve the original style, tone, and atmosphere.
5. Japanese book-style formatting:

   * Insert **one line break between paragraphs**.
   * At the beginning of each paragraph, insert **one full-width space character**ï¼ˆå…¨è§’ã‚¹ãƒšãƒ¼ã‚¹1æ–‡å­—åˆ†ï¼‰.
   * Remove unnecessary line breaks inside sentences.

**Output requirements:**

* æœ€çµ‚å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã§æ›¸ãã€æ•´ç†ã•ã‚ŒãŸå®Œæˆç‰ˆã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
* å­—æ•°ã¯å¿…ãš3000å­—ä»¥ä¸Š4000å­—ä»¥å†…ã«ã—ã¦ãã ã•ã„ã€‚
* å‡ºåŠ›ã¯æœ¬æ–‡ã®å†…å®¹ã®ã¿ã¨ã—ã€ä½™è¨ˆãªèª¬æ˜ã‚„æ³¨é‡ˆã€æ³¨æ„æ›¸ãã€ãƒ¡ã‚¿ã‚³ãƒ¡ãƒ³ãƒˆã¯ä¸€åˆ‡ä»˜ã‘ãªã„ã§ãã ã•ã„ã€‚
"""

# =========================
# LLM
# =========================
def _is_llm_available() -> bool:
    return (
        genai is not None
        and types is not None
        and bool(os.getenv("GEMINI_API_KEY"))
        and bool(os.getenv("GEMINI_MODEL"))
    )

def call_model():
    if genai is None or types is None:
        raise ImportError("google-genai ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install google-genai` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise ValueError("'GEMINI_API_KEY' setting is missing in environment variables.")

    client = genai.Client(api_key=api_key)
    model = os.getenv("GEMINI_MODEL")
    if not model:
        raise ValueError("'GEMINI_MODEL' setting is missing in environment variables.")
    return client, model

def format_text(raw_text: str) -> str:
    client, model = call_model()
    full_prompt = f"{format_prompt}\n\nHere is the raw text:\n{raw_text}"
    contents = [
        types.Content(
            role="user",
            parts=[types.Part.from_text(text=full_prompt)],
        )
    ]
    response = client.models.generate_content(model=model, contents=contents)
    return response.text

def summarize_text(formatted_text: str, chapter_title: str) -> str:
    client, model = call_model()
    summary_prompt = summary_prompt_template.format(chapter_title=chapter_title)
    full_prompt = f"{summary_prompt}\n\nHere is the formatted text:\n{formatted_text}"
    contents = [
        types.Content(
            role="user",
            parts=[types.Part.from_text(text=full_prompt)],
        )
    ]
    response = client.models.generate_content(model=model, contents=contents)
    response_text = response.text.strip()

    heading_line = f"## {chapter_title}".strip()
    lines = response_text.splitlines()
    if lines:
        first_line = lines[0].strip()
        if first_line.startswith("##"):
            lines[0] = heading_line
        else:
            lines.insert(0, heading_line)
    else:
        lines = [heading_line]

    body_lines = [line.rstrip() for line in lines[1:]]

    while body_lines and not body_lines[0].strip():
        body_lines.pop(0)
    while body_lines and not body_lines[-1].strip():
        body_lines.pop()

    normalized_body = "\n".join(body_lines)
    if normalized_body:
        final_text = f"{heading_line}\n\n{normalized_body}"
    else:
        final_text = heading_line

    return final_text

def _build_google_doc_content(sections: List[Tuple[str, str]]) -> str:
    """
    Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”¨ã«ç« ã”ã¨ã®æœ¬æ–‡ã‚’é€£çµã€‚
    """
    blocks: List[str] = []
    for raw_title, raw_body in sections:
        heading = raw_title.strip() or "ç„¡é¡Œ"
        body = (raw_body or "").strip()
        if body.startswith("##"):
            body_lines = body.splitlines()
            first_line = body_lines[0].strip()
            normalized_heading = f"## {heading}"
            if first_line == normalized_heading:
                body_lines = body_lines[1:]
                while body_lines and not body_lines[0].strip():
                    body_lines.pop(0)
                body = "\n".join(body_lines).strip()
        block_parts = [heading]
        if body:
            block_parts.append(body)
        blocks.append("\n".join(block_parts))
    return "\n\n\n".join(blocks).strip() or "æœ¬æ–‡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"

def _get_drive_service(creds):
    if build is None:
        raise ImportError("googleapiclient ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install google-api-python-client` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    return build("drive", "v3", credentials=creds)

def _find_folder_id(service, name: str, parent_id: Optional[str]) -> Optional[str]:
    """è¦ªIDç›´ä¸‹ã«ã‚ã‚‹ name ã®ãƒ•ã‚©ãƒ«ãƒ€IDã‚’ã²ã¨ã¤è¿”ã™ï¼ˆæœ€åˆã®ä¸€è‡´ï¼‰ã€‚è¦ªãªã—ã®å ´åˆã¯ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–ç›´ä¸‹ã‚’æ¤œç´¢ã€‚"""
    # Drive ã® query ã¯å˜å¼•ç”¨ç¬¦ã§å›²ã‚€ã®ã§ã€å˜å¼•ç”¨ç¬¦ã¯ãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã§ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
    safe_name = (name or "").replace("'", "\\'")
    name_q = "name = '{}'".format(safe_name)
    mime_q = "mimeType = 'application/vnd.google-apps.folder'"
    trashed_q = "trashed = false"

    if parent_id:
        parent_q = f"'{parent_id}' in parents"
        q = f"{name_q} and {mime_q} and {trashed_q} and {parent_q}"
    else:
        q = f"{name_q} and {mime_q} and {trashed_q}"

    res = service.files().list(q=q, fields="files(id, name, parents)").execute()
    files = res.get("files", [])
    if not files:
        return None
    if parent_id:
        return files[0]["id"]
    return files[0]["id"]

def _resolve_book_folder_id(creds, root_name: str, book_title: str) -> Optional[str]:
    """root_name/book_title ã®ãƒ•ã‚©ãƒ«ãƒ€IDã‚’æ¨å®šã—ã¦è¿”ã™ï¼ˆå­˜åœ¨ã—ãªã‘ã‚Œã° Noneï¼‰ã€‚"""
    try:
        service = _get_drive_service(creds)
        root_id = _find_folder_id(service, root_name, parent_id=None)
        if not root_id:
            return None
        book_id = _find_folder_id(service, book_title, parent_id=root_id)
        return book_id
    except Exception:
        return None

def _folder_url(folder_id: str) -> str:
    return f"https://drive.google.com/drive/folders/{folder_id}"

def _search_url(book_title: str) -> str:
    # è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã¨ãã®ä¿é™ºã¨ã—ã¦æ¤œç´¢URLã‚’æç¤º
    from urllib.parse import quote
    return f"https://drive.google.com/drive/search?q={quote(book_title)}"


def _build_single_doc_content(heading: str, body: str) -> str:
    """
    1ã¤ã®ãƒ‘ãƒ¼ãƒˆã‚’1ã¤ã®Google Docã«æ›¸ãå‡ºã™ãŸã‚ã®æœ¬æ–‡ã‚’ç”Ÿæˆã€‚
    body å…ˆé ­ã« '## è¦‹å‡ºã—' ãŒä»˜ã„ã¦ã„ã‚‹å ´åˆã¯é‡è¤‡ã—ãªã„ã‚ˆã†é™¤å»ã€‚
    """
    h = (heading or "ç„¡é¡Œ").strip()
    b = (body or "").strip()

    if b.startswith("##"):
        lines = b.splitlines()
        if lines and lines[0].strip() == f"## {h}":
            lines = lines[1:]
            while lines and not lines[0].strip():
                lines.pop(0)
            b = "\n".join(lines).strip()

    return f"{h}\n\n{b}" if b else h


def _make_part_doc_name(idx: int) -> str:
    """
    ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåï¼ˆPart 01.doc ãªã©ï¼‰
    Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè‡ªä½“ã¯ãƒã‚¤ãƒ†ã‚£ãƒ–å½¢å¼ã§ã™ãŒã€åå‰ã« .doc ã‚’å«ã‚ã¦ã‚‚å•é¡Œã‚ã‚Šã¾ã›ã‚“ã€‚
    """
    return f"Part {idx:02d}.doc"

def _make_part_summary_doc_name(idx: int) -> str:
    """
    è¦ç´„ç”¨ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåï¼ˆPart1 _è¦ç´„.doc ãªã©ï¼‰
    â€» æ•°å­—ã¯ã‚¼ãƒ­åŸ‹ã‚ã—ãªã„ï¼Part ã¨æ•°å­—ã®é–“ã¯è©°ã‚ã‚‹ï¼æ•°å­—ã®å¾Œã«åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ï¼‹"_è¦ç´„.doc"
    """
    return f"Part{idx} _è¦ç´„.doc"


def _get_cached_google_credentials():
    # ã“ã“ã§ get_google_credentials ãŒ None ã®å¯èƒ½æ€§ã¯ãªã„ï¼ˆä¸Šã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§å®šç¾©æ¸ˆã¿ï¼‰
    creds = st.session_state.get("google_creds")
    if creds is None:
        creds = get_google_credentials()
        st.session_state.google_creds = creds
    return creds


def normalize_drive_folder_input(raw_value: str) -> str:
    """Driveã®ãƒ•ã‚©ãƒ«ãƒ€URLã‹ã‚‰ folderId ã‚’æŠ½å‡ºã€‚IDãªã‚‰ãã®ã¾ã¾è¿”ã™ã€‚"""
    if not raw_value:
        return raw_value
    raw_value = raw_value.strip()
    match = re.search(r"/folders/([a-zA-Z0-9_-]+)", raw_value)
    if match:
        return match.group(1)
    match = re.search(r"id=([a-zA-Z0-9_-]+)", raw_value)
    if match:
        return match.group(1)
    return raw_value

def _list_drive_images(creds, folder_id: str) -> List[Dict[str, str]]:
    if build is None:
        raise ImportError("googleapiclient ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install google-api-python-client` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    service = build("drive", "v3", credentials=creds)
    query = f"'{folder_id}' in parents and mimeType contains 'image/' and trashed = false"
    fields = "nextPageToken, files(id, name, mimeType, modifiedTime)"
    page_token = None
    files: List[Dict[str, str]] = []
    while True:
        response = service.files().list(
            q=query,
            spaces="drive",
            fields=fields,
            orderBy="name_natural",
            pageToken=page_token,
        ).execute()
        files.extend(response.get("files", []))
        page_token = response.get("nextPageToken")
        if not page_token:
            break
    return files

def _download_drive_images(creds, file_entries: List[Dict[str, str]], dest_dir: str) -> List[str]:
    if build is None or MediaIoBaseDownload is None:
        raise ImportError("googleapiclient ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install google-api-python-client` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    os.makedirs(dest_dir, exist_ok=True)
    service = build("drive", "v3", credentials=creds)
    saved_paths: List[str] = []
    for entry in file_entries:
        file_id = entry.get("id")
        filename = entry.get("name") or f"{file_id}.img"
        base_name, ext = os.path.splitext(filename)
        ext = ext if ext else ".jpg"
        safe_base = re.sub(r"[^\w\-.ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥]", "_", base_name)[:100]
        safe_name = f"{safe_base}{ext}"
        dest_path = os.path.join(dest_dir, safe_name)
        counter = 1
        while os.path.exists(dest_path):
            dest_path = os.path.join(dest_dir, f"{safe_base}_{counter}{ext}")
            counter += 1

        request = service.files().get_media(fileId=file_id)
        fh = io.BytesIO()
        downloader = MediaIoBaseDownload(fh, request)
        done = False
        while not done:
            _, done = downloader.next_chunk()
        with open(dest_path, "wb") as f:
            f.write(fh.getvalue())
        saved_paths.append(dest_path)
    return saved_paths

def _log_drive_identity_once(creds):
    """Drive APIã§ç¾åœ¨ã®èªè¨¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ä¸€åº¦ã ã‘è¡¨ç¤º"""
    if st.session_state.get("whoami_logged"):
        return
    try:
        from googleapiclient.discovery import build
        me = build("drive", "v3", credentials=creds).about().get(
            fields="user(emailAddress)"
        ).execute()
        email = me["user"]["emailAddress"]
        st.info(f"Google Drive ã¨ã—ã¦å®Ÿè¡Œä¸­: **{email}**")
        st.session_state.whoami_logged = True
    except Exception as e:
        st.warning(f"èªè¨¼ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡¨ç¤ºã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        st.session_state.whoami_logged = True  # å¤±æ•—ã—ã¦ã‚‚ãƒ«ãƒ¼ãƒ—é˜²æ­¢


# =========================
# Utility
# =========================
def save_uploaded_images(files, workdir: str) -> List[str]:
    os.makedirs(workdir, exist_ok=True)
    saved = []
    for f in files:
        # iOSã®live photoæ‹¡å¼µå­å¯¾ç­–å«ã‚€æ‹¡å¼µå­æ­£è¦åŒ–
        suffix = os.path.splitext(f.name)[1].lower()
        suffix = ".jpg" if suffix in {".jpeg", ".jpg"} else ".png" if suffix in {".png"} else ".jpg"
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼‹å…ƒåã§è¡çªå›é¿
        fname = f"{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}_{os.path.basename(f.name)}"
        path = os.path.join(workdir, fname)
        # PILçµŒç”±ã§ç”»åƒã ã‘ä¿å­˜ï¼ˆHEICç­‰ã¯åˆ¥é€”ãƒ©ã‚¤ãƒ–ãƒ©ãƒªå¿…è¦ï¼‰
        img = Image.open(f).convert("RGB")
        img.save(path if suffix == ".jpg" else path.replace(".jpg", ".png"))
        saved.append(path if suffix == ".jpg" else path.replace(".jpg", ".png"))
    return saved

def normalize_filenames_local(paths: List[str]) -> List[str]:
    """
    åå‰ä¸­ã® 'ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ YYYY-MM-DD HH.MM.SS [é€£ç•ª?]' ã‚’ä¸¦ã¹æ›¿ãˆã€‚
    ãƒ•ã‚¡ã‚¤ãƒ«åãŒä¸Šè¨˜è¦å‰‡ã§ãªã„å ´åˆã¯mtimeã§ã‚½ãƒ¼ãƒˆã€‚
    """
    def parse_key(p: str) -> Tuple[int, str]:
        base = os.path.splitext(os.path.basename(p))[0]
        m = re.match(r"(ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ (\d{4}-\d{2}-\d{2}) (\d{2}\.\d{2}\.\d{2}))(?: (\d+))?$", base)
        if m:
            dt = f"{m.group(2)} {m.group(3).replace('.',':')}"
            num = int(m.group(4)) if m.group(4) else 0
            try:
                ts = int(datetime.strptime(dt, "%Y-%m-%d %H:%M:%S").timestamp())
            except ValueError:
                ts = int(os.path.getmtime(p))
            return (ts * 100 + num, p)
        else:
            return (int(os.path.getmtime(p)) * 100, p)

    sorted_paths = sorted(paths, key=parse_key)
    return sorted_paths

# OCRçµæœdictã‹ã‚‰æœ¬æ–‡ã‚’å …ç‰¢ã«æŠœãå‡ºã™
def _pick_text(d: Dict[str, Any]) -> str:
    """OCR/å¤–éƒ¨æŠ½å‡ºã®çµæœ dict ã‹ã‚‰æœ¬æ–‡æ–‡å­—åˆ—ã‚’å …ç‰¢ã«å–ã‚Šå‡ºã™"""
    if not isinstance(d, dict):
        return ""
    for k in ("Text", "text", "Body", "body", "FullText", "full_text", "content"):
        v = d.get(k)
        if isinstance(v, str) and v.strip():
            return v
    # Google Vision ã® raw ã‚’ãã®ã¾ã¾å…¥ã‚ŒãŸå ´åˆã®ä¿é™º
    v = d.get("full_text_annotation", {}).get("text") if isinstance(d.get("full_text_annotation"), dict) else None
    return v.strip() if isinstance(v, str) else ""

# =========================
# OCR
# =========================
@st.cache_resource(show_spinner=False)
# def get_vision_client(json_key_path: Optional[str] = None):
#     # json_key_pathãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°ä¸€æ™‚çš„ã«ç’°å¢ƒå¤‰æ•°ã‚’å·®ã—æ›¿ãˆï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³å­˜ç¶šä¸­ã®ã¿ï¼‰
#     if json_key_path:
#         os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = json_key_path
#     from google.cloud import vision
#     credentials_info = json.loads(st.secrets["GOOGLE_CREDENTIALS"])
#     st.write(credentials_info)
#     credentials = service_account.Credentials.from_service_account_info(credentials_info)
#     client = vision.ImageAnnotatorClient(credentials=credentials)
#     # client = _vision_client_from_secrets(credentials=credentials)
    
#     return client

def get_vision_client(json_key_path: Optional[str] = None):
    if json_key_path:
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = json_key_path

    # â† è¿½åŠ ï¼ˆç’°å¢ƒå¤‰æ•°ã§ quota project ãŒå‹æ‰‹ã«ä»˜ãã®ã‚’é˜²ãï¼‰
    for k in ("GOOGLE_CLOUD_QUOTA_PROJECT", "GOOGLE_CLOUD_PROJECT", "GCLOUD_PROJECT"):
        os.environ.pop(k, None)

    from google.cloud import vision
    import json
    from google.oauth2 import service_account

    raw = st.secrets["GOOGLE_CREDENTIALS"]
    credentials_info = json.loads(raw) if isinstance(raw, str) else raw

    # ã‚¹ã‚³ãƒ¼ãƒ—ã‚’æ˜ç¤ºã—ã¦ä½œã‚‹ï¼ˆæ¨å¥¨ï¼‰
    scopes = ["https://www.googleapis.com/auth/cloud-platform"]
    credentials = service_account.Credentials.from_service_account_info(credentials_info, scopes=scopes)

    # â˜… ã“ã“ã§ with_quota_project ã¯ä»˜ã‘ãªã„ï¼ˆä»Šã¯ä¸è¦ï¼‰
    # credentials = credentials.with_quota_project("sunny-advantage-471612-v1")

    client = vision.ImageAnnotatorClient(credentials=credentials)
    return client


def _extract_with_vision(img_path: str, client) -> Dict[str, Any]:
    from google.cloud import vision
    with open(img_path, "rb") as f:
        content = f.read()
    image = vision.Image(content=content)
    # å’Œæ›¸ã®ç¸¦æ›¸ãã‚’å«ã‚€æ—¥æœ¬èªãƒ’ãƒ³ãƒˆ
    context = vision.ImageContext(language_hints=["ja"])
    resp = client.document_text_detection(image=image, image_context=context)
    if resp.error.message:
        raise RuntimeError(resp.error.message)
    return resp

def fix_broken_chapter_tokens(text: str) -> str:
    """
    OCRã§ã€Œç¬¬ 1 ç« ã€ã€Œãƒ— ãƒ­ ãƒ­ ãƒ¼ ã‚°ã€ãªã©ã«å‰²ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¿®å¾©
    """
    t = text
    # å…¨è§’/åŠè§’çµ±ä¸€
    t = unicodedata.normalize("NFKC", t)
    # é€£ç¶šç©ºç™½ã®é™¤å»ï¼ˆãŸã ã—æ”¹è¡Œã¯æ®‹ã™ï¼‰
    t = re.sub(r"[ \t\u3000]+", " ", t)
    # ã€Œç¬¬ X ç« ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã®éš™é–“ã‚’æ½°ã™
    t = re.sub(r"ç¬¬\s*([0-9ï¼-ï¼™ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒã€‡é›¶]+)\s*ç« ", r"ç¬¬\1ç« ", t)
    # ãƒ—ãƒ­ãƒ­ãƒ¼ã‚°/ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°/åºç« /çµ‚ç« /çµè«–
    for token in ["ãƒ—ãƒ­ãƒ­ãƒ¼ã‚°", "ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°", "åºç« ", "çµ‚ç« ", "çµè«–", "ã‚ã¨ãŒã"]:
        t = re.sub(r"(" + r"\s*".join(list(token)) + r")", token, t)
    return t

# ç« ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè¡Œé ­é™å®šï¼‰æ¤œå‡ºï¼ˆæ®‹ã—ã¦ã„ã‚‹ãŒç¾è¡Œã¯ä½¿ã‚ãªã„ï¼‰
CHAPTER_RE = re.compile(
    r"^(?P<heading>"
    r"ç¬¬(?:[1-9][0-9]*|[ï¼-ï¼™]+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒã€‡é›¶]+)ç« (?:[ \t\u3000ï¼š:\-ãƒ»][^\n]*)?"
    r"|(?:ãƒ—ãƒ­ãƒ­ãƒ¼ã‚°|åºç« |çµ‚ç« |ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°|çµè«–|ã‚ã¨ãŒã)(?:[ \t\u3000ï¼š:\-ãƒ»][^\n]*)?"
    r")$",
    re.MULTILINE,
)

def split_by_chapter_linehead(text: str) -> List[Tuple[str, str]]:
    """
    ç« è¦‹å‡ºã—ã‚’è¡Œé ­é™å®šã§åˆ†å‰²ï¼ˆæœ¬æ–‡ä¸­ã®ã€Œç¬¬Xç« ã€ã«ã¯åå¿œã—ãªã„ï¼‰
    """
    text = fix_broken_chapter_tokens(text)
    parts: List[Tuple[str, str]] = []
    current_title = None
    buff: List[str] = []
    for line in text.splitlines():
        m = CHAPTER_RE.match(line.strip())
        if m:
            if current_title and buff:
                parts.append((current_title, "\n".join(buff).strip()))
            current_title = m.group("heading").strip()
            buff = []
        else:
            buff.append(line)
    if current_title and buff:
        parts.append((current_title, "\n".join(buff).strip()))
    return parts if parts else [("æœ¬æ–‡", text.strip())]

def split_by_fixed_chars(text: str, size: int = 10000) -> List[Tuple[str, str]]:
    """
    æœ¬æ–‡ã‚’å›ºå®šæ–‡å­—æ•° size ã”ã¨ã«åˆ†å‰²ã™ã‚‹ã€‚
    æœ«å°¾ã§ã¡ã‚‡ã†ã©åˆ‡ã‚Œãªã„å ´åˆã¯ã€ã§ãã‚‹ã ã‘è¿‘ã„æ®µè½å¢ƒç•Œï¼ˆ\n\nï¼‰ã¾ã§æˆ»ã—ã¦åˆ‡ã‚‹ã€‚
    ãã‚Œã§ã‚‚è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ãã®ã¾ã¾ size ã§åˆ‡ã‚‹ã€‚
    è¿”ã‚Šå€¤ã¯ [(ã‚¿ã‚¤ãƒˆãƒ«, æœ¬æ–‡)] ã®ã‚¿ãƒ—ãƒ«é…åˆ—ï¼ˆå¾Œå·¥ç¨‹äº’æ›ã®ãŸã‚ã‚¿ã‚¤ãƒˆãƒ«ã‚’ä»˜ä¸ï¼‰ã€‚
    """
    t = text or ""
    n = len(t)
    parts: List[Tuple[str, str]] = []
    i = 0
    idx = 1
    while i < n:
        end = min(i + size, n)
        # ã§ãã‚Œã°æ®µè½åŒºåˆ‡ã‚Šã§åˆ‡ã‚‹ï¼ˆå¾Œæ–¹ 1200 æ–‡å­—ä»¥å†…ã‚’æ¢ç´¢ï¼‰
        if end < n:
            window_start = max(i, end - 1200)
            cut_pos = t.rfind("\n\n", window_start, end)
            if cut_pos != -1 and cut_pos > i + 1000:  # ã‚ã¾ã‚Šæ‰‹å‰ã§åˆ‡ã‚Œã™ããªã„ã‚ˆã†ã«è»½ã„ã‚¬ãƒ¼ãƒ‰
                end = cut_pos
        chunk = t[i:end].strip()
        title = f"Part {idx:02d} ({i+1:,}â€“{end:,})"
        parts.append((title, chunk))
        i = end
        idx += 1
    return parts if parts else [("Part 01", t)]

def simple_info_extractor(full_text: str) -> Dict[str, Optional[str]]:
    """
    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ç°¡æ˜“infoæŠ½å‡ºï¼ˆRight/Leftã‚’ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ‹¾ã†ï¼‰
    """
    t = unicodedata.normalize("NFKC", full_text)
    right = None
    m = re.search(r"(\d{1,3})\s*%", t)
    if m:
        right = f"{m.group(1)}%"

    left = None
    m2 = re.search(r"(æœ¬ã‚’èª­ã¿çµ‚ãˆã‚‹ã¾ã§\d+åˆ†|[0-9ï¼-ï¼™]+ãƒšãƒ¼ã‚¸ä¸­[0-9ï¼-ï¼™]+ãƒšãƒ¼ã‚¸)", t)
    if m2:
        left = m2.group(1)

    return {"Title": None, "Subtitle": None, "Right": right, "Left": left}

def ocr_one_image(img_path: str, client) -> Dict[str, Any]:
    """
    1ç”»åƒã®OCRâ†’infoæŠ½å‡ºâ†’æœ¬æ–‡è¿”å´
    """
    if extract_info_external:
        # å¤–éƒ¨å®Ÿè£…ã®æˆ»ã‚ŠãŒ Body/Text ãªã©æ§˜ã€…ã§ã‚‚ Text ã‚’å¿…ãšåŸ‹ã‚ã‚‹
        res = extract_info_external(img_path)
        res = dict(res) if isinstance(res, dict) else {}
        text = _pick_text(res)
        res.setdefault("Filename", os.path.basename(img_path))
        res["Text"] = text
        return res

    # ç„¡ã„å ´åˆã¯æ±ç”¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    resp = _extract_with_vision(img_path, client)
    full_text = ""
    if getattr(resp, "full_text_annotation", None) and resp.full_text_annotation.text:
        full_text = resp.full_text_annotation.text
    elif getattr(resp, "text_annotations", None):
        full_text = resp.text_annotations[0].description

    info = simple_info_extractor(full_text)
    return {
        "Filename": os.path.basename(img_path),
        "Text": full_text,
        **info
    }

def best_effort_summarize(chapter_title: str, chapter_text: str) -> str:
    """
    LLMãŒåˆ©ç”¨å¯èƒ½ãªã‚‰æ•´å½¢â†’è¦ç´„ã‚’å®Ÿæ–½ã—ã€ä¸å¯ãªã‚‰ç´ æœ´ã«çŸ­ç¸®ã€‚
    """
    if _is_llm_available():
        try:
            formatted = format_text(chapter_text)
            return summarize_text(formatted, chapter_title)
        except Exception as e:
            st.warning(f"LLMã«ã‚ˆã‚‹è¦ç´„å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå…ˆé ­ã‚’ã„ã„æ„Ÿã˜ã«è¦ç´„é¢¨ã‚µãƒãƒª
    trimmed = re.sub(r"\s+", " ", chapter_text).strip()
    return trimmed[:1200] + ("â€¦" if len(trimmed) > 1200 else "")

# =========================
# Streamlit UI
# =========================
st.set_page_config(page_title="Kindleæ›¸ç± è‡ªå‹•è¦ç´„ãƒ„ãƒ¼ãƒ« (MVP)", layout="wide")

st.title("ğŸ“š Kindleæ›¸ç± è‡ªå‹•è¦ç´„ãƒ„ãƒ¼ãƒ«")
st.caption("ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ â†’ ä¸¦ã¹æ›¿ãˆ â†’ OCR â†’ ç« /å›ºå®šé•·åˆ†å‰² â†’ è¦ç´„ â†’ Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡ºåŠ› ã¾ã§")

with st.sidebar:
    st.header("è¨­å®š")
    st.write("Google Cloud èªè¨¼")
    cred_mode = st.radio("èªè¨¼æ–¹æ³•", ["ç’°å¢ƒå¤‰æ•°ã‚’ä½¿ã†", "JSONã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"], horizontal=True)
    uploaded_key = None
    if cred_mode == "JSONã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
        key_file = st.file_uploader("ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®JSON", type=["json"])
        if key_file is not None:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".json")
            tmp.write(key_file.read())
            tmp.flush()
            uploaded_key = tmp.name
            st.success("èªè¨¼æƒ…å ±ã‚’ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")

    st.divider()
    st.write("è¦ç´„ã®é•·ã•ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ï¼‰")
    max_chars = st.slider("è¦ç´„ä¸Šé™æ–‡å­—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰", 600, 2000, 1200, 100)
    st.session_state.max_chars = max_chars

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹
if "workdir" not in st.session_state:
    st.session_state.workdir = tempfile.mkdtemp(prefix="kindle_ocr_")
if "images" not in st.session_state:
    st.session_state.images = []
if "ocr_results" not in st.session_state:
    st.session_state.ocr_results = []
if "full_text" not in st.session_state:
    st.session_state.full_text = ""
if "chapters" not in st.session_state:
    st.session_state.chapters = []
if "summaries" not in st.session_state:
    st.session_state.summaries = {}
if "drive_folder_input" not in st.session_state:
    st.session_state.drive_folder_input = ""
if "drive_loaded_folder_id" not in st.session_state:
    st.session_state.drive_loaded_folder_id = None
if "drive_files" not in st.session_state:
    st.session_state.drive_files = []
if "needs_chapter_split" not in st.session_state:
    st.session_state.needs_chapter_split = False

# Step 1: ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
st.subheader("Step 1. ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
files = st.file_uploader("Kindleã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰: JPG/PNG", type=["jpg","jpeg","png"], accept_multiple_files=True)
col1, col2 = st.columns([1,1])
with col1:
    if st.button("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¿å­˜", use_container_width=True) and files:
        saved = save_uploaded_images(files, st.session_state.workdir)
        st.session_state.images.extend(saved)
        st.success(f"{len(saved)}æšã®ç”»åƒã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")

with col2:
    if st.button("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ç”»åƒã‚’è¡¨ç¤º", use_container_width=True):
        st.write(f"ä¿å­˜å…ˆ: `{st.session_state.workdir}`")
        for p in st.session_state.images[:12]:
            st.image(p, width=220)
        if len(st.session_state.images) > 12:
            st.caption(f"â€¦ã»ã‹ {len(st.session_state.images) - 12} æš")

with st.expander("Googleãƒ‰ãƒ©ã‚¤ãƒ–ã‹ã‚‰å–å¾—", expanded=False):
    st.write("Google Drive ã®ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰ç›´æ¥ç”»åƒã‚’èª­ã¿è¾¼ã¿ã€Step 1 ã«è¿½åŠ ã—ã¾ã™ã€‚")
    drive_folder_value = st.text_input(
        "ãƒ•ã‚©ãƒ«ãƒ€ID ã¾ãŸã¯ URL",
        key="drive_folder_input",
        placeholder="ä¾‹: https://drive.google.com/drive/folders/xxxxxxxxxxxxxxxxx",
    )
    if st.button("ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ç”»åƒã‚’èª­ã¿è¾¼ã‚€", use_container_width=True):
        folder_id = normalize_drive_folder_input(drive_folder_value)
        if not folder_id:
            st.warning("ãƒ•ã‚©ãƒ«ãƒ€IDã¾ãŸã¯URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            if folder_id == st.session_state.get("drive_loaded_folder_id"):
                st.info("åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã¯æ—¢ã«èª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã™ã€‚Step 2 ã«é€²ã‚“ã§ãã ã•ã„ã€‚")
            else:
                try:
                    creds = _get_cached_google_credentials()
                    with st.spinner("Google Drive ã‹ã‚‰ç”»åƒã‚’å–å¾—ä¸­..."):
                        files = _list_drive_images(creds, folder_id)
                        st.session_state.drive_files = files
                        if not files:
                            st.info("æŒ‡å®šã—ãŸãƒ•ã‚©ãƒ«ãƒ€ã«ã¯ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                        else:
                            saved_paths = _download_drive_images(creds, files, st.session_state.workdir)
                            existing = set(st.session_state.images)
                            new_paths = [p for p in saved_paths if p not in existing]
                            if not new_paths:
                                st.info("æ–°ã—ã„ç”»åƒã¯è¿½åŠ ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                            else:
                                st.session_state.images.extend(new_paths)
                                st.session_state.drive_loaded_folder_id = folder_id
                                st.success(f"{len(new_paths)} ä»¶ã®ç”»åƒã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚Step 2 ã§ä¸¦ã³æ›¿ãˆã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
                except Exception as e:
                    st.error(f"ãƒ•ã‚©ãƒ«ãƒ€ã®èª­ã¿è¾¼ã¿ã¾ãŸã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

# Step 2: ä¸¦ã³æ›¿ãˆï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å/æ™‚åˆ»ãƒ™ãƒ¼ã‚¹ï¼‰
st.subheader("Step 2. ãƒšãƒ¼ã‚¸é †ã«ä¸¦ã³æ›¿ãˆ")
if st.session_state.images:
    if st.button("ä¸¦ã³æ›¿ãˆã‚’å®Ÿè¡Œ", use_container_width=True):
        if normalize_filenames_external:
            normalize_filenames_external(st.session_state.workdir)
            # â†‘ ã‚ãªãŸã®é–¢æ•°ã®è¿”ã‚Šå€¤ä»•æ§˜ã«åˆã‚ã›ã¦èª¿æ•´ãŒå¿…è¦ãªå ´åˆã‚ã‚Š
            # ã“ã“ã§ã¯workdirå†…ã‚’ãƒªãƒãƒ¼ãƒ â†’å†å–å¾—ã‚’æƒ³å®š
            st.session_state.images = [os.path.join(st.session_state.workdir, f) for f in os.listdir(st.session_state.workdir)]
        st.session_state.images = normalize_filenames_local(st.session_state.images)
        st.success("ä¸¦ã³æ›¿ãˆå®Œäº†")
    # st.caption("â€» ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å‘½åè¦å‰‡ãŒå³å¯†ã«æ±ºã¾ã£ã¦ã„ã‚‹å ´åˆã¯ã€`normalize_filenames` ã®å®Ÿè£…ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")

# Step 3: OCR & infoæŠ½å‡º
st.subheader("Step 3. OCR & infoæŠ½å‡º")
if st.session_state.images:
    client = get_vision_client(uploaded_key)
    if st.button("OCRã‚’å®Ÿè¡Œ", type="primary", use_container_width=True):
        results = []
        prog = st.progress(0.0, text="OCRå‡¦ç†ä¸­â€¦")
        for i, p in enumerate(st.session_state.images):
            try:
                info = ocr_one_image(p, client)
                info["Path"] = p
                results.append(info)
            except Exception as e:
                st.error(f"OCRå¤±æ•—: {os.path.basename(p)} â€” {e}")
            prog.progress((i+1)/len(st.session_state.images), text=f"OCRå‡¦ç†ä¸­â€¦ {i+1}/{len(st.session_state.images)}")
        st.session_state.ocr_results = results

        # 1ã¤ã®æœ¬æ–‡ã«é€£çµï¼ˆãƒšãƒ¼ã‚¸é–“ã«æ”¹è¡ŒæŒ¿å…¥ï¼‰â€” ã‚­ãƒ¼å·®ç•°ã‚’å¸å
        texts = []
        for r in results:
            t = _pick_text(r)
            if t:
                texts.append(t)
        combined_text = "\n\n".join(texts).strip()

        # â˜… ã“ã“ã§ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é©ç”¨ï¼ˆè¦‹å‡ºã—ä¿®æ­£/ãƒ¬ãƒ³ã‚¸è¡Œå‰Šé™¤/æ–‡ä¸­æ”¹è¡Œè§£æ¶ˆï¼‰
        cleaned_text = clean_presummary_text(combined_text)

        st.session_state.full_text = cleaned_text
        st.session_state.needs_chapter_split = bool(st.session_state.full_text)

        st.success(f"OCRå®Œäº†ï¼š{len(results)}ãƒšãƒ¼ã‚¸")

if st.session_state.ocr_results:
    with st.expander("æŠ½å‡ºçµæœï¼ˆæœ€åˆã®æ•°ä»¶ï¼‰", expanded=False):
        # ä¸­èº«ã‚’ç›®è¦–ç¢ºèªã—ã‚„ã™ã„ã‚ˆã† snippet ã‚’è¡¨ç¤º
        preview = []
        for r in st.session_state.ocr_results[:3]:
            preview.append({
                "Filename": r.get("Filename"),
                "Snippet": (_pick_text(r) or "")[:120]
            })
        st.json(preview)

# Step 4: æ–‡å­—æ•°ã§åˆ†å‰²ï¼ˆ10,000å­—ã”ã¨ï¼‰
st.subheader("Step 4. æ–‡å­—æ•°ã§åˆ†å‰²ï¼ˆ10,000å­—ã”ã¨ï¼‰")

# Step 3 ã®çµæœã‹ã‚‰ full_text ãŒç©ºãªã‚‰å¾©å…ƒï¼ˆã‚­ãƒ¼å·®ç•°ã‚’å¸åï¼‰
if st.session_state.ocr_results and not (st.session_state.full_text or "").strip():
    reconstructed = "\n\n".join([_pick_text(r) for r in st.session_state.ocr_results if _pick_text(r)]).strip()
    if reconstructed:
        # â˜… å¾©å…ƒæ™‚ã«ã‚‚ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°é©ç”¨
        reconstructed = clean_presummary_text(reconstructed)
        st.session_state.full_text = reconstructed
        if not st.session_state.chapters:
            st.session_state.needs_chapter_split = True  # å¤‰æ•°åã¯äº’æ›åˆ©ç”¨

full_text_value = (st.session_state.get("full_text") or "").strip()
# åˆ†å‰²ã¯æœ¬æ–‡ãŒã‚ã‚‹ã¨ãã®ã¿æœ‰åŠ¹ï¼ˆç©ºåˆ†å‰²ã‚’é˜²æ­¢ï¼‰
can_split = bool(full_text_value)

# st.caption(f"full_text length = {len(full_text_value)}")  # â†ãƒ‡ãƒãƒƒã‚°ç”¨ã«å¿…è¦ã§ã‚ã‚Œã°ã‚³ãƒ¡ãƒ³ãƒˆè§£é™¤

col_step4_run, col_step4_clear = st.columns([2, 1])
with col_step4_run:
    run_split_clicked = st.button(
        "10,000å­—ã”ã¨ã«åˆ†å‰²ã‚’å®Ÿè¡Œ",
        use_container_width=True,
        disabled=not can_split,
        key="run_char_split",
    )
with col_step4_clear:
    clear_split_clicked = st.button(
        "åˆ†å‰²çµæœã‚’ã‚¯ãƒªã‚¢",
        use_container_width=True,
        disabled=not bool(st.session_state.chapters),
        key="clear_char_split",
    )

if run_split_clicked:
    try:
        # ç« ãƒˆãƒ¼ã‚¯ãƒ³ä¿®å¾©ã¯ä¸è¦ã ãŒã€OCRã®ãƒã‚¤ã‚ºæ•´å½¢ã¨ã—ã¦æ®‹ã—ã¦ã‚‚å®³ã¯ãªã„
        fixed = fix_broken_chapter_tokens(st.session_state.full_text)
        parts = split_by_fixed_chars(fixed, size=10000)
        st.session_state.chapters = parts            # ä¸‹æµäº’æ›ã®ãŸã‚åŒã˜ã‚­ãƒ¼ã«å…¥ã‚Œã‚‹
        st.session_state.needs_chapter_split = False
        st.success(f"ç”Ÿæˆãƒãƒ£ãƒ³ã‚¯æ•°: {len(parts)}")
    except Exception as e:
        st.error(f"æ–‡å­—æ•°åˆ†å‰²ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

if clear_split_clicked:
    st.session_state.chapters = []
    st.session_state.needs_chapter_split = False
    st.info("åˆ†å‰²çµæœã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")

if st.session_state.get("needs_chapter_split") and can_split and not st.session_state.get("chapters"):
    try:
        fixed = fix_broken_chapter_tokens(st.session_state.full_text)
        parts = split_by_fixed_chars(fixed, size=10000)
        st.session_state.chapters = parts
        st.session_state.needs_chapter_split = False
        if parts:
            st.success(f"ç”Ÿæˆãƒãƒ£ãƒ³ã‚¯æ•°: {len(parts)}")
        else:
            st.warning("æœ¬æ–‡ãŒç©ºã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚Step 3 ã® OCR ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        st.error(f"æ–‡å­—æ•°åˆ†å‰²ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        st.session_state.needs_chapter_split = False

if st.session_state.chapters:
    st.success(f"ç”Ÿæˆãƒãƒ£ãƒ³ã‚¯æ•°: {len(st.session_state.chapters)}")
    with st.expander("ãƒãƒ£ãƒ³ã‚¯ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå…ˆé ­3ã¤ï¼‰", expanded=False):
        for title, body in st.session_state.chapters[:3]:
            st.markdown(f"### {title}")
            st.text(body[:800] + ("\nâ€¦(ä»¥ä¸‹ç•¥)" if len(body) > 800 else ""))
elif not can_split:
    st.info("Step 3 ã§ OCR ã‚’å®Ÿè¡Œã—æœ¬æ–‡ã‚’å–å¾—ã—ã¦ãã ã•ã„ã€‚")

# Step 5: è¦ç´„
st.subheader("Step 5. è¦ç´„ç”Ÿæˆ")
if st.session_state.chapters:
    # ã¾ã¨ã‚ã¦è¦ç´„
    if st.button("å…¨ç« ã‚’è¦ç´„ã™ã‚‹", type="primary", use_container_width=True):
        st.session_state.summaries = {}
        for idx, (title, body) in enumerate(st.session_state.chapters, start=1):
            with st.spinner(f"{idx}/{len(st.session_state.chapters)} è¦ç´„ä¸­: {title}"):
                summary = best_effort_summarize(title, body)
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é•·åˆ¶å¾¡
                if not _is_llm_available():
                    summary = summary[:st.session_state.get("max_chars", 1200)]
                st.session_state.summaries[title] = summary
        st.success("å…¨ç« ã®è¦ç´„ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    if st.session_state.summaries:
        with st.expander("è¦ç´„çµæœï¼ˆä¸Šä½3ç« ï¼‰", expanded=True):
            for i, (title, summ) in enumerate(list(st.session_state.summaries.items())[:3], start=1):
                #st.markdown(f"## {title}")
                st.write(summ)

# Step 6: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
st.subheader("Step 6. ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
if st.session_state.summaries:
    st.write("Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å‡ºåŠ›ã—ã¾ã™ã€‚åˆå›ã¯ãƒ–ãƒ©ã‚¦ã‚¶ã§Googleèªè¨¼ãŒæ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚")
    default_book_title = st.session_state.get("book_title_input", "Kindleæ›¸ç±")
    default_root = st.session_state.get("drive_root_input", "OCRçµæœ")
    book_title_input = st.text_input("æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆGoogleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåã«ä½¿ç”¨ï¼‰", value=default_book_title)
    drive_root_input = st.text_input("Google Driveã®ä¿å­˜å…ˆãƒ«ãƒ¼ãƒˆãƒ•ã‚©ãƒ«ãƒ€", value=default_root)
    st.session_state.book_title_input = book_title_input
    st.session_state.drive_root_input = drive_root_input

    st.markdown("**å‡ºåŠ›æ–¹æ³•**")
    export_mode = st.radio(
        "Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä½œã‚Šæ–¹",
        ["1æœ¬ã«ã¾ã¨ã‚ã‚‹ï¼ˆå¾“æ¥ï¼‰", "ãƒ‘ãƒ¼ãƒˆã”ã¨ã«åˆ†å‰²ã™ã‚‹ï¼ˆPart 01.doc / Part 02.doc ...ï¼‰"],
        horizontal=False,
        index=1,  # æ—¢å®šã§ã€Œåˆ†å‰²ã€
    )
    split_summaries = st.checkbox("è¦ç´„ã‚‚ãƒ‘ãƒ¼ãƒˆã”ã¨ã«å€‹åˆ¥ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§å‡ºåŠ›ã™ã‚‹", value=True)

    if st.button("Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆ", type="primary", use_container_width=True):
        if create_google_doc_external is None or get_google_credentials is None:
            st.error("google_api_utils.py ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡ºåŠ›ã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ã€‚")
        else:
            try:
                creds = st.session_state.get("google_creds")
                if creds is None:
                    with st.spinner("Googleã‚¢ã‚«ã‚¦ãƒ³ãƒˆèªè¨¼ä¸­â€¦"):
                        # creds = get_google_credentials()
                        creds = get_google_credentials(use_user_oauth=True)
                    st.session_state.google_creds = creds
                _log_drive_identity_once(creds)
                
                # ç« /ãƒ‘ãƒ¼ãƒˆå€™è£œï¼ˆStep4ã®çµæœãŒç„¡ã‘ã‚Œã°å…¨æ–‡ã‚’1ä»¶ã¨ã—ã¦æ‰±ã†ï¼‰
                chapters_for_doc = (
                    st.session_state.chapters
                    if st.session_state.chapters
                    else [("æœ¬æ–‡", st.session_state.full_text or "")]
                )

                if export_mode.startswith("1æœ¬ã«ã¾ã¨ã‚ã‚‹"):
                    # å¾“æ¥ã®ã¾ã¨ã‚æ›¸ãå‡ºã—
                    full_content = _build_google_doc_content(chapters_for_doc)
                    summary_sections = list(st.session_state.summaries.items())
                    summary_content = _build_google_doc_content(summary_sections)

                    with st.spinner("æ–‡ç« å…¨ä½“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆä¸­â€¦"):
                        create_google_doc_external(
                            book_title_input,
                            "æ–‡ç« å…¨ä½“",
                            full_content,
                            creds,
                            root_name=drive_root_input or "OCRçµæœ",
                        )
                    with st.spinner("è¦ç´„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆä¸­â€¦"):
                        create_google_doc_external(
                            book_title_input,
                            "è¦ç´„",
                            summary_content,
                            creds,
                            root_name=drive_root_input or "OCRçµæœ",
                        )
                    st.success("Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚Google Drive ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
                    # â–¼ ã“ã“ã‹ã‚‰ï¼šãƒ•ã‚©ãƒ«ãƒ€ãƒªãƒ³ã‚¯è¡¨ç¤º
                    folder_id = _resolve_book_folder_id(creds, drive_root_input or "OCRçµæœ", book_title_input)
                    if folder_id:
                        st.markdown(f"ğŸ“‚ ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€: [{book_title_input}]({_folder_url(folder_id)})")
                    else:
                        st.markdown(
                            "ğŸ“‚ ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã‚’è‡ªå‹•ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
                            f" æ¤œç´¢ã¯ã“ã¡ã‚‰ â†’ [{book_title_input}]({_search_url(book_title_input)})"
                        )

                else:
                    # â˜… ãƒ‘ãƒ¼ãƒˆã”ã¨ã«åˆ†å‰²ã—ã¦æ›¸ãå‡ºã— â˜…
                    total = len(chapters_for_doc)

                    # æœ¬æ–‡ã®åˆ†å‰²å‡ºåŠ›
                    with st.spinner("ãƒ‘ãƒ¼ãƒˆã”ã¨ã®æœ¬æ–‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆä¸­â€¦"):
                        for idx, (title, body) in enumerate(chapters_for_doc, start=1):
                            doc_name = _make_part_doc_name(idx)  # ä¾‹: Part 01.doc
                            content = _build_single_doc_content(title, body)
                            create_google_doc_external(
                                book_title_input,
                                doc_name,
                                content,
                                creds,
                                root_name=drive_root_input or "OCRçµæœ",
                            )

                    # è¦ç´„ã®åˆ†å‰²å‡ºåŠ›ï¼ˆä»»æ„ï¼‰
                    if split_summaries and st.session_state.summaries:
                        with st.spinner("ãƒ‘ãƒ¼ãƒˆã”ã¨ã®è¦ç´„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆä¸­â€¦"):
                            for idx, (title, _) in enumerate(chapters_for_doc, start=1):
                                summary_text = st.session_state.summaries.get(title)
                                if not summary_text:
                                    continue  # ãã®ã‚¿ã‚¤ãƒˆãƒ«ã®è¦ç´„ãŒç„¡ã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                                # è¦ç´„ã¯è¦‹å‡ºã—é‡è¤‡ã‚’é¿ã‘ã¤ã¤ã€ã‚¿ã‚¤ãƒˆãƒ«è¡Œã¯æœ¬æ–‡åŒ–ã—ã¦ãŠã
                                content = _build_single_doc_content(title, summary_text)
                                doc_name = _make_part_summary_doc_name(idx)  # ä¾‹: Part 01.doc
                                # è¦ç´„ã¨æœ¬æ–‡ã§åŒåã«ã—ãŸããªã„å ´åˆã¯ä¸‹è¡Œã«å¤‰æ›´ä¾‹ï¼š
                                # doc_name = f"Part {idx:02d}ï¼ˆè¦ç´„ï¼‰.doc"
                                create_google_doc_external(
                                    book_title_input,
                                    doc_name,
                                    content,
                                    creds,
                                    root_name=drive_root_input or "OCRçµæœ/è¦ç´„",
                                )

                    st.success(f"ãƒ‘ãƒ¼ãƒˆåˆ†å‰²ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼ˆ{total}ä»¶ï¼‰ã€‚Google Drive ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
                    # â–¼ ã“ã“ã‹ã‚‰ï¼šãƒ•ã‚©ãƒ«ãƒ€ãƒªãƒ³ã‚¯è¡¨ç¤º
                    folder_id = _resolve_book_folder_id(creds, drive_root_input or "OCRçµæœ", book_title_input)
                    if folder_id:
                        st.markdown(f"ğŸ“‚ ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€: [{book_title_input}]({_folder_url(folder_id)})")
                    else:
                        st.markdown(
                            "ğŸ“‚ ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã‚’è‡ªå‹•ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
                            f" æ¤œç´¢ã¯ã“ã¡ã‚‰ â†’ [{book_title_input}]({_search_url(book_title_input)})"
                        )

            except Exception as e:
                st.error(f"Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
