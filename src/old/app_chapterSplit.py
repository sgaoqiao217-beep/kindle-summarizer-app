import io
import os
import re
import tempfile
import unicodedata
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional

import streamlit as st
from PIL import Image
from dotenv import load_dotenv

try:
    from google import genai
    from google.genai import types
except Exception:
    genai = None
    types = None

try:
    from googleapiclient.discovery import build
    from googleapiclient.http import MediaIoBaseDownload
except Exception:
    build = None
    MediaIoBaseDownload = None

try:
    from google_api_utils import (
        get_credentials as get_google_credentials,
        create_google_doc as create_google_doc_external,
    )
except Exception:
    get_google_credentials = None
    create_google_doc_external = None

# Optional: æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒã‚ã‚Œã°ä½¿ã†ï¼ˆç„¡ã‘ã‚Œã°ImportErrorã‚’æ¡ã‚Šã¤ã¶ã—ã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
try:
    from sort_image_file import normalize_filenames as normalize_filenames_external
except Exception:
    normalize_filenames_external = None

try:
    from ocr_utils import extract_info_type1 as extract_info_external
    # â†‘ ãƒ¦ãƒ¼ã‚¶ç’°å¢ƒã®é–¢æ•°åã«åˆã‚ã›ã¦ãŠå¥½ã¿ã§
except Exception:
    extract_info_external = None

load_dotenv()

format_prompt = """
You are given raw text from a Japanese book (a novel or story).
Your task is ONLY to format the text into a clean book-like structure.

Rules:
1. Insert one line break between paragraphs.
2. At the beginning of each paragraph, insert one full-width space character (å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹1æ–‡å­—åˆ†).
3. Remove unnecessary line breaks inside sentences.
4. If there are unnatural word breaks or typos caused by OCR, fix them using context.
5. Do NOT summarize or shorten the content. Keep all original content.

æœ€çµ‚å‡ºåŠ›ã¯æœ¬æ–‡ã®æ•´å½¢å¾Œãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
ä½™è¨ˆãªèª¬æ˜ã‚„æ³¨é‡ˆã¯ä¸€åˆ‡ä¸è¦ã§ã™ã€‚
"""

summary_prompt_template = """
You are given a chapter from a Japanese novel or story (already formatted).
Your task is to summarize it into a polished, story-like text.

Use the following chapter title exactly as provided:
{chapter_title}

**Output structure (must follow exactly):**

1. First line: the chapter title as an H2 heading.
   * If Google Docs: apply **HEADING_2** style to the title line.
2. One blank line.
3. The summary body text (Japanese), 3000â€“4000 characters, formatted per the rules below.

**Critical constraints (do not violate):**

* Do **not** stop after writing the heading. Writing only the heading is invalid.
* Always include the full summary body after the blank line.
* If the input lacks a clear chapter title, infer a concise title from the content (e.g., main topic or scene) and still output it as H2.

**Rules for the summary body:**

1. Length: **3000â€“4000 Japanese characters** (strict).
2. Include all key events, characters, and emotional flow.
3. Ensure the writing is natural, grammatically correct, and coherent.
4. Preserve the original style, tone, and atmosphere.
5. Japanese book-style formatting:

   * Insert **one line break between paragraphs**.
   * At the beginning of each paragraph, insert **one full-width space character**ï¼ˆå…¨è§’ã‚¹ãƒšãƒ¼ã‚¹1æ–‡å­—åˆ†ï¼‰.
   * Remove unnecessary line breaks inside sentences.

**Output requirements:**

* æœ€çµ‚å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã§æ›¸ãã€æ•´ç†ã•ã‚ŒãŸå®Œæˆç‰ˆã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
* å­—æ•°ã¯å¿…ãš3000å­—ä»¥ä¸Š4000å­—ä»¥å†…ã«ã—ã¦ãã ã•ã„ã€‚
* å‡ºåŠ›ã¯æœ¬æ–‡ã®å†…å®¹ã®ã¿ã¨ã—ã€ä½™è¨ˆãªèª¬æ˜ã‚„æ³¨é‡ˆã€æ³¨æ„æ›¸ãã€ãƒ¡ã‚¿ã‚³ãƒ¡ãƒ³ãƒˆã¯ä¸€åˆ‡ä»˜ã‘ãªã„ã§ãã ã•ã„ã€‚
"""


def _is_llm_available() -> bool:
    return (
        genai is not None
        and types is not None
        and bool(os.getenv("GEMINI_API_KEY"))
        and bool(os.getenv("GEMINI_MODEL"))
    )


def call_model():
    if genai is None or types is None:
        raise ImportError("google-genai ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install google-genai` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise ValueError("'GEMINI_API_KEY' setting is missing in environment variables.")

    client = genai.Client(api_key=api_key)
    model = os.getenv("GEMINI_MODEL")
    if not model:
        raise ValueError("'GEMINI_MODEL' setting is missing in environment variables.")
    return client, model


def format_text(raw_text: str) -> str:
    client, model = call_model()
    full_prompt = f"{format_prompt}\n\nHere is the raw text:\n{raw_text}"
    contents = [
        types.Content(
            role="user",
            parts=[types.Part.from_text(text=full_prompt)],
        )
    ]
    response = client.models.generate_content(model=model, contents=contents)
    return response.text


def summarize_text(formatted_text: str, chapter_title: str) -> str:
    client, model = call_model()
    summary_prompt = summary_prompt_template.format(chapter_title=chapter_title)
    full_prompt = f"{summary_prompt}\n\nHere is the formatted text:\n{formatted_text}"
    contents = [
        types.Content(
            role="user",
            parts=[types.Part.from_text(text=full_prompt)],
        )
    ]
    response = client.models.generate_content(model=model, contents=contents)
    response_text = response.text.strip()

    heading_line = f"## {chapter_title}".strip()
    lines = response_text.splitlines()
    if lines:
        first_line = lines[0].strip()
        if first_line.startswith("##"):
            lines[0] = heading_line
        else:
            lines.insert(0, heading_line)
    else:
        lines = [heading_line]

    body_lines = [line.rstrip() for line in lines[1:]]

    while body_lines and not body_lines[0].strip():
        body_lines.pop(0)
    while body_lines and not body_lines[-1].strip():
        body_lines.pop()

    normalized_body = "\n".join(body_lines)
    if normalized_body:
        final_text = f"{heading_line}\n\n{normalized_body}"
    else:
        final_text = heading_line

    return final_text


def _build_google_doc_content(sections: List[Tuple[str, str]]) -> str:
    """
    Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”¨ã«ç« ã”ã¨ã®æœ¬æ–‡ã‚’é€£çµã€‚
    """
    blocks: List[str] = []
    for raw_title, raw_body in sections:
        heading = raw_title.strip() or "ç„¡é¡Œ"
        body = (raw_body or "").strip()
        if body.startswith("##"):
            body_lines = body.splitlines()
            first_line = body_lines[0].strip()
            normalized_heading = f"## {heading}"
            if first_line == normalized_heading:
                body_lines = body_lines[1:]
                while body_lines and not body_lines[0].strip():
                    body_lines.pop(0)
                body = "\n".join(body_lines).strip()
        block_parts = [heading]
        if body:
            block_parts.append(body)
        blocks.append("\n".join(block_parts))
    return "\n\n\n".join(blocks).strip() or "æœ¬æ–‡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"


def _get_cached_google_credentials():
    if get_google_credentials is None:
        raise RuntimeError("google_api_utils ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚Googleèªè¨¼ã‚’åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
    creds = st.session_state.get("google_creds")
    if creds is None:
        creds = get_google_credentials()
        st.session_state.google_creds = creds
    return creds


def _extract_drive_folder_id(raw_value: str) -> Optional[str]:
    if not raw_value:
        return None
    raw_value = raw_value.strip()
    match = re.search(r"/folders/([a-zA-Z0-9_-]+)", raw_value)
    if match:
        return match.group(1)
    match = re.search(r"id=([a-zA-Z0-9_-]+)", raw_value)
    if match:
        return match.group(1)
    return raw_value


def _list_drive_images(creds, folder_id: str) -> List[Dict[str, str]]:
    if build is None:
        raise ImportError("googleapiclient ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install google-api-python-client` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    service = build("drive", "v3", credentials=creds)
    query = f"'{folder_id}' in parents and mimeType contains 'image/' and trashed = false"
    fields = "nextPageToken, files(id, name, mimeType, modifiedTime)"
    page_token = None
    files: List[Dict[str, str]] = []
    while True:
        response = service.files().list(
            q=query,
            spaces="drive",
            fields=fields,
            orderBy="name_natural",
            pageToken=page_token,
        ).execute()
        files.extend(response.get("files", []))
        page_token = response.get("nextPageToken")
        if not page_token:
            break
    return files


def _download_drive_images(creds, file_entries: List[Dict[str, str]], dest_dir: str) -> List[str]:
    if build is None or MediaIoBaseDownload is None:
        raise ImportError("googleapiclient ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install google-api-python-client` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    os.makedirs(dest_dir, exist_ok=True)
    service = build("drive", "v3", credentials=creds)
    saved_paths: List[str] = []
    for entry in file_entries:
        file_id = entry.get("id")
        filename = entry.get("name") or f"{file_id}.img"
        base_name, ext = os.path.splitext(filename)
        ext = ext if ext else ".jpg"
        safe_base = re.sub(r"[^\w\-.ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥]", "_", base_name)[:100]
        safe_name = f"{safe_base}{ext}"
        dest_path = os.path.join(dest_dir, safe_name)
        counter = 1
        while os.path.exists(dest_path):
            dest_path = os.path.join(dest_dir, f"{safe_base}_{counter}{ext}")
            counter += 1

        request = service.files().get_media(fileId=file_id)
        fh = io.BytesIO()
        downloader = MediaIoBaseDownload(fh, request)
        done = False
        while not done:
            _, done = downloader.next_chunk()
        with open(dest_path, "wb") as f:
            f.write(fh.getvalue())
        saved_paths.append(dest_path)
    return saved_paths

# =========================
# Utility
# =========================
def save_uploaded_images(files, workdir: str) -> List[str]:
    os.makedirs(workdir, exist_ok=True)
    saved = []
    for f in files:
        # iOSã®live photoæ‹¡å¼µå­å¯¾ç­–å«ã‚€æ‹¡å¼µå­æ­£è¦åŒ–
        suffix = os.path.splitext(f.name)[1].lower()
        suffix = ".jpg" if suffix in {".jpeg", ".jpg"} else ".png" if suffix in {".png"} else ".jpg"
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼‹å…ƒåã§è¡çªå›é¿
        fname = f"{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}_{os.path.basename(f.name)}"
        path = os.path.join(workdir, fname)
        # PILçµŒç”±ã§ç”»åƒã ã‘ä¿å­˜ï¼ˆHEICç­‰ã¯åˆ¥é€”ãƒ©ã‚¤ãƒ–ãƒ©ãƒªå¿…è¦ï¼‰
        img = Image.open(f).convert("RGB")
        img.save(path if suffix == ".jpg" else path.replace(".jpg", ".png"))
        saved.append(path if suffix == ".jpg" else path.replace(".jpg", ".png"))
    return saved

def normalize_filenames_local(paths: List[str]) -> List[str]:
    """
    åå‰ä¸­ã® 'ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ YYYY-MM-DD HH.MM.SS [é€£ç•ª?]' ã‚’ä¸¦ã¹æ›¿ãˆã€‚
    ãƒ•ã‚¡ã‚¤ãƒ«åãŒä¸Šè¨˜è¦å‰‡ã§ãªã„å ´åˆã¯mtimeã§ã‚½ãƒ¼ãƒˆã€‚
    """
    def parse_key(p: str) -> Tuple[int, str]:
        base = os.path.splitext(os.path.basename(p))[0]
        m = re.match(r"(ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ (\d{4}-\d{2}-\d{2}) (\d{2}\.\d{2}\.\d{2}))(?: (\d+))?$", base)
        if m:
            dt = f"{m.group(2)} {m.group(3).replace('.',':')}"
            num = int(m.group(4)) if m.group(4) else 0
            try:
                ts = int(datetime.strptime(dt, "%Y-%m-%d %H:%M:%S").timestamp())
            except ValueError:
                ts = int(os.path.getmtime(p))
            return (ts * 100 + num, p)
        else:
            return (int(os.path.getmtime(p)) * 100, p)

    sorted_paths = sorted(paths, key=parse_key)
    return sorted_paths

# =========================
# OCR
# =========================
@st.cache_resource(show_spinner=False)
def get_vision_client(json_key_path: Optional[str] = None):
    # json_key_pathãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°ä¸€æ™‚çš„ã«ç’°å¢ƒå¤‰æ•°ã‚’å·®ã—æ›¿ãˆï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³å­˜ç¶šä¸­ã®ã¿ï¼‰
    if json_key_path:
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = json_key_path
    from google.cloud import vision
    return vision.ImageAnnotatorClient()

def _extract_with_vision(img_path: str, client) -> Dict[str, Any]:
    from google.cloud import vision
    with open(img_path, "rb") as f:
        content = f.read()
    image = vision.Image(content=content)
    # å’Œæ›¸ã®ç¸¦æ›¸ãã‚’å«ã‚€æ—¥æœ¬èªãƒ’ãƒ³ãƒˆ
    context = vision.ImageContext(language_hints=["ja"])
    resp = client.document_text_detection(image=image, image_context=context)
    if resp.error.message:
        raise RuntimeError(resp.error.message)
    return resp

def fix_broken_chapter_tokens(text: str) -> str:
    """
    OCRã§ã€Œç¬¬ 1 ç« ã€ã€Œãƒ— ãƒ­ ãƒ­ ãƒ¼ ã‚°ã€ãªã©ã«å‰²ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¿®å¾©
    """
    t = text
    # å…¨è§’/åŠè§’çµ±ä¸€
    t = unicodedata.normalize("NFKC", t)
    # é€£ç¶šç©ºç™½ã®é™¤å»ï¼ˆãŸã ã—æ”¹è¡Œã¯æ®‹ã™ï¼‰
    t = re.sub(r"[ \t\u3000]+", " ", t)
    # ã€Œç¬¬ X ç« ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã®éš™é–“ã‚’æ½°ã™
    t = re.sub(r"ç¬¬\s*([0-9ï¼-ï¼™ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒã€‡é›¶]+)\s*ç« ", r"ç¬¬\1ç« ", t)
    # ãƒ—ãƒ­ãƒ­ãƒ¼ã‚°/ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°/åºç« /çµ‚ç« /çµè«–
    for token in ["ãƒ—ãƒ­ãƒ­ãƒ¼ã‚°", "ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°", "åºç« ", "çµ‚ç« ", "çµè«–", "ã‚ã¨ãŒã"]:
        t = re.sub(r"(" + r"\s*".join(list(token)) + r")", token, t)
    return t

# ç« ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè¡Œé ­é™å®šï¼‰æ¤œå‡ºï¼šéå»ã«å…±æœ‰ã—ã¦ãã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ™ãƒ¼ã‚¹ã«æ”¹è‰¯
CHAPTER_RE = re.compile(
    r"^(?P<heading>"
    r"ç¬¬(?:[1-9][0-9]*|[ï¼-ï¼™]+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒã€‡é›¶]+)ç« (?:[ \t\u3000ï¼š:\-ãƒ»][^\n]*)?"
    r"|(?:ãƒ—ãƒ­ãƒ­ãƒ¼ã‚°|åºç« |çµ‚ç« |ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°|çµè«–|ã‚ã¨ãŒã)(?:[ \t\u3000ï¼š:\-ãƒ»][^\n]*)?"
    r")$",
    re.MULTILINE,
)

def split_by_chapter_linehead(text: str) -> List[Tuple[str, str]]:
    """
    ç« è¦‹å‡ºã—ã‚’è¡Œé ­é™å®šã§åˆ†å‰²ï¼ˆæœ¬æ–‡ä¸­ã®ã€Œç¬¬Xç« ã€ã«ã¯åå¿œã—ãªã„ï¼‰
    """
    text = fix_broken_chapter_tokens(text)
    parts: List[Tuple[str, str]] = []
    current_title = None
    buff: List[str] = []
    for line in text.splitlines():
        m = CHAPTER_RE.match(line.strip())
        if m:
            # ç›´å‰ã‚’ç¢ºå®š
            if current_title and buff:
                parts.append((current_title, "\n".join(buff).strip()))
            current_title = m.group("heading").strip()
            buff = []
        else:
            buff.append(line)
    if current_title and buff:
        parts.append((current_title, "\n".join(buff).strip()))
    return parts if parts else [("æœ¬æ–‡", text.strip())]

def simple_info_extractor(full_text: str) -> Dict[str, Optional[str]]:
    """
    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ç°¡æ˜“infoæŠ½å‡ºï¼ˆRight/Leftã‚’ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ‹¾ã†ï¼‰
    """
    t = unicodedata.normalize("NFKC", full_text)
    right = None
    m = re.search(r"(\d{1,3})\s*%", t)
    if m:
        right = f"{m.group(1)}%"

    left = None
    m2 = re.search(r"(æœ¬ã‚’èª­ã¿çµ‚ãˆã‚‹ã¾ã§\d+åˆ†|[0-9ï¼-ï¼™]+ãƒšãƒ¼ã‚¸ä¸­[0-9ï¼-ï¼™]+ãƒšãƒ¼ã‚¸)", t)
    if m2:
        left = m2.group(1)

    return {"Title": None, "Subtitle": None, "Right": right, "Left": left}

def ocr_one_image(img_path: str, client) -> Dict[str, Any]:
    """
    1ç”»åƒã®OCRâ†’infoæŠ½å‡ºâ†’æœ¬æ–‡è¿”å´
    """
    if extract_info_external:
        # ã‚ãªãŸã®é«˜ç²¾åº¦é–¢æ•°ãŒã‚ã‚‹å ´åˆã¯ã“ã¡ã‚‰ã‚’å„ªå…ˆ
        return extract_info_external(img_path)

    # ç„¡ã„å ´åˆã¯æ±ç”¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    resp = _extract_with_vision(img_path, client)
    full_text = ""
    if resp.full_text_annotation and resp.full_text_annotation.text:
        full_text = resp.full_text_annotation.text
    elif getattr(resp, "text_annotations", None):
        full_text = resp.text_annotations[0].description

    info = simple_info_extractor(full_text)
    return {
        "Filename": os.path.basename(img_path),
        "Text": full_text,
        **info
    }

def best_effort_summarize(chapter_title: str, chapter_text: str) -> str:
    """
    LLMãŒåˆ©ç”¨å¯èƒ½ãªã‚‰æ•´å½¢â†’è¦ç´„ã‚’å®Ÿæ–½ã—ã€ä¸å¯ãªã‚‰ç´ æœ´ã«çŸ­ç¸®ã€‚
    """
    if _is_llm_available():
        try:
            formatted = format_text(chapter_text)
            return summarize_text(formatted, chapter_title)
        except Exception as e:
            st.warning(f"LLMã«ã‚ˆã‚‹è¦ç´„å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå…ˆé ­ã‚’ã„ã„æ„Ÿã˜ã«è¦ç´„é¢¨ã‚µãƒãƒª
    trimmed = re.sub(r"\s+", " ", chapter_text).strip()
    return trimmed[:1200] + ("â€¦" if len(trimmed) > 1200 else "")

# =========================
# Streamlit UI
# =========================
st.set_page_config(page_title="Kindleæ›¸ç± è‡ªå‹•è¦ç´„ãƒ„ãƒ¼ãƒ« (MVP)", layout="wide")

st.title("ğŸ“š Kindleæ›¸ç± è‡ªå‹•è¦ç´„ãƒ„ãƒ¼ãƒ«")
st.caption("ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ â†’ ä¸¦ã¹æ›¿ãˆ â†’ OCR â†’ ç« æ¤œå‡º â†’ è¦ç´„ â†’ Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡ºåŠ› ã¾ã§")

with st.sidebar:
    st.header("è¨­å®š")
    st.write("Google Cloud èªè¨¼")
    cred_mode = st.radio("èªè¨¼æ–¹æ³•", ["ç’°å¢ƒå¤‰æ•°ã‚’ä½¿ã†", "JSONã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"], horizontal=True)
    uploaded_key = None
    if cred_mode == "JSONã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
        key_file = st.file_uploader("ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®JSON", type=["json"])
        if key_file is not None:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".json")
            tmp.write(key_file.read())
            tmp.flush()
            uploaded_key = tmp.name
            st.success("èªè¨¼æƒ…å ±ã‚’ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")

    st.divider()
    st.write("è¦ç´„ã®é•·ã•ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ï¼‰")
    max_chars = st.slider("è¦ç´„ä¸Šé™æ–‡å­—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰", 600, 2000, 1200, 100)
    st.session_state.max_chars = max_chars

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹
if "workdir" not in st.session_state:
    st.session_state.workdir = tempfile.mkdtemp(prefix="kindle_ocr_")
if "images" not in st.session_state:
    st.session_state.images = []
if "ocr_results" not in st.session_state:
    st.session_state.ocr_results = []
if "full_text" not in st.session_state:
    st.session_state.full_text = ""
if "chapters" not in st.session_state:
    st.session_state.chapters = []
if "summaries" not in st.session_state:
    st.session_state.summaries = {}
if "drive_folder_input" not in st.session_state:
    st.session_state.drive_folder_input = ""
if "drive_loaded_folder_id" not in st.session_state:
    st.session_state.drive_loaded_folder_id = None
if "drive_files" not in st.session_state:
    st.session_state.drive_files = []
if "needs_chapter_split" not in st.session_state:
    st.session_state.needs_chapter_split = False

# Step 1: ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
st.subheader("Step 1. ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
files = st.file_uploader("Kindleã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰: JPG/PNG", type=["jpg","jpeg","png"], accept_multiple_files=True)
col1, col2 = st.columns([1,1])
with col1:
    if st.button("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¿å­˜", use_container_width=True) and files:
        saved = save_uploaded_images(files, st.session_state.workdir)
        st.session_state.images.extend(saved)
        st.success(f"{len(saved)}æšã®ç”»åƒã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")

with col2:
    if st.button("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ç”»åƒã‚’è¡¨ç¤º", use_container_width=True):
        st.write(f"ä¿å­˜å…ˆ: `{st.session_state.workdir}`")
        for p in st.session_state.images[:12]:
            st.image(p, width=220)
        if len(st.session_state.images) > 12:
            st.caption(f"â€¦ã»ã‹ {len(st.session_state.images) - 12} æš")

with st.expander("Googleãƒ‰ãƒ©ã‚¤ãƒ–ã‹ã‚‰å–å¾—", expanded=False):
    st.write("Google Drive ã®ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰ç›´æ¥ç”»åƒã‚’èª­ã¿è¾¼ã¿ã€Step 1 ã«è¿½åŠ ã—ã¾ã™ã€‚")
    drive_folder_value = st.text_input(
        "ãƒ•ã‚©ãƒ«ãƒ€ID ã¾ãŸã¯ URL",
        key="drive_folder_input",
        placeholder="ä¾‹: https://drive.google.com/drive/folders/xxxxxxxxxxxxxxxxx",
    )
    if st.button("ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ç”»åƒã‚’èª­ã¿è¾¼ã‚€", use_container_width=True):
        folder_id = _extract_drive_folder_id(drive_folder_value)
        if not folder_id:
            st.warning("ãƒ•ã‚©ãƒ«ãƒ€IDã¾ãŸã¯URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            if folder_id == st.session_state.get("drive_loaded_folder_id"):
                st.info("åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã¯æ—¢ã«èª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã™ã€‚Step 2 ã«é€²ã‚“ã§ãã ã•ã„ã€‚")
            else:
                try:
                    creds = _get_cached_google_credentials()
                    with st.spinner("Google Drive ã‹ã‚‰ç”»åƒã‚’å–å¾—ä¸­..."):
                        files = _list_drive_images(creds, folder_id)
                        st.session_state.drive_files = files
                        if not files:
                            st.info("æŒ‡å®šã—ãŸãƒ•ã‚©ãƒ«ãƒ€ã«ã¯ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                        else:
                            saved_paths = _download_drive_images(creds, files, st.session_state.workdir)
                            existing = set(st.session_state.images)
                            new_paths = [p for p in saved_paths if p not in existing]
                            if not new_paths:
                                st.info("æ–°ã—ã„ç”»åƒã¯è¿½åŠ ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                            else:
                                st.session_state.images.extend(new_paths)
                                st.session_state.drive_loaded_folder_id = folder_id
                                st.success(f"{len(new_paths)} ä»¶ã®ç”»åƒã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚Step 2 ã§ä¸¦ã³æ›¿ãˆã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
                except Exception as e:
                    st.error(f"ãƒ•ã‚©ãƒ«ãƒ€ã®èª­ã¿è¾¼ã¿ã¾ãŸã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

# Step 2: ä¸¦ã³æ›¿ãˆï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å/æ™‚åˆ»ãƒ™ãƒ¼ã‚¹ï¼‰
st.subheader("Step 2. ãƒšãƒ¼ã‚¸é †ã«ä¸¦ã³æ›¿ãˆ")
if st.session_state.images:
    colA, colB = st.columns([1,1])
    with colA:
        st.write("ä¸¦ã³æ›¿ãˆæ–¹å¼")
        how = st.radio("ãƒ«ãƒ¼ãƒ«", ["ã‚ãªãŸã®`normalize_filenames`ã‚’ä½¿ç”¨", "MVPå†…ã®ç°¡æ˜“ãƒ«ãƒ¼ãƒ«"], horizontal=False)
    with colB:
        if st.button("ä¸¦ã³æ›¿ãˆã‚’å®Ÿè¡Œ", use_container_width=True):
            if how == "ã‚ãªãŸã®`normalize_filenames`ã‚’ä½¿ç”¨" and normalize_filenames_external:
                sorted_list = normalize_filenames_external(st.session_state.workdir)
                # â†‘ ã‚ãªãŸã®é–¢æ•°ã®è¿”ã‚Šå€¤ä»•æ§˜ã«åˆã‚ã›ã¦èª¿æ•´ãŒå¿…è¦ãªå ´åˆã‚ã‚Š
                # ã“ã“ã§ã¯workdirå†…ã‚’ãƒªãƒãƒ¼ãƒ â†’å†å–å¾—ã‚’æƒ³å®š
                st.session_state.images = [os.path.join(st.session_state.workdir, f) for f in os.listdir(st.session_state.workdir)]
                st.session_state.images = normalize_filenames_local(st.session_state.images)
            else:
                st.session_state.images = normalize_filenames_local(st.session_state.images)
            st.success("ä¸¦ã³æ›¿ãˆå®Œäº†")
    st.caption("â€» ã‚‚ã—ã‚ãªãŸã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å‘½åè¦å‰‡ãŒå³å¯†ã«æ±ºã¾ã£ã¦ã„ã‚‹å ´åˆã¯ã€å¤–éƒ¨é–¢æ•°ã®å‘¼ã³å‡ºã—ã‚’å„ªå…ˆã—ã¦ãã ã•ã„ã€‚")

# Step 3: OCR & infoæŠ½å‡º
st.subheader("Step 3. OCR & infoæŠ½å‡º")
if st.session_state.images:
    client = get_vision_client(uploaded_key)
    if st.button("OCRã‚’å®Ÿè¡Œ", type="primary", use_container_width=True):
        results = []
        prog = st.progress(0.0, text="OCRå‡¦ç†ä¸­â€¦")
        for i, p in enumerate(st.session_state.images):
            try:
                info = ocr_one_image(p, client)
                info["Path"] = p
                results.append(info)
            except Exception as e:
                st.error(f"OCRå¤±æ•—: {os.path.basename(p)} â€” {e}")
            prog.progress((i+1)/len(st.session_state.images), text=f"OCRå‡¦ç†ä¸­â€¦ {i+1}/{len(st.session_state.images)}")
        st.session_state.ocr_results = results
        # 1ã¤ã®æœ¬æ–‡ã«é€£çµï¼ˆãƒšãƒ¼ã‚¸é–“ã«æ”¹è¡ŒæŒ¿å…¥ï¼‰
        combined_text = "\n\n".join([r.get("Text", "") for r in results if (r.get("Text") or "").strip()]).strip()
        st.session_state.full_text = combined_text
        st.session_state.needs_chapter_split = bool(st.session_state.full_text or st.session_state.ocr_results)
        st.success(f"OCRå®Œäº†ï¼š{len(results)}ãƒšãƒ¼ã‚¸")
        # UI ã‚’ç¢ºå®Ÿã«æœ€æ–°çŠ¶æ…‹ã«ã™ã‚‹
        st.rerun()

if st.session_state.ocr_results:
    with st.expander("æŠ½å‡ºçµæœï¼ˆæœ€åˆã®æ•°ä»¶ï¼‰", expanded=False):
        st.json(st.session_state.ocr_results[:3])

# Step 4: ç« åˆ†å‰²
st.subheader("Step 4. ç« åˆ†å‰²")
if st.session_state.ocr_results and not (st.session_state.full_text or "").strip():
    reconstructed = "\n\n".join(
        [r.get("Text", "") for r in st.session_state.ocr_results if (r.get("Text") or "").strip()]
    ).strip()
    if reconstructed:
        st.session_state.full_text = reconstructed
        if not st.session_state.chapters:
            st.session_state.needs_chapter_split = True

full_text_value = (st.session_state.get("full_text") or "").strip()
has_full_text = bool(full_text_value.strip())
has_ocr_text = any(bool((r.get("Text") or "").strip()) for r in st.session_state.get("ocr_results", []))
#can_split_chapters = bool(full_text_value or has_ocr_text)
can_split_chapters = bool(full_text_value or st.session_state.get("ocr_results"))
col_step4_run, col_step4_clear = st.columns([2, 1])
with col_step4_run:
    run_split_clicked = st.button(
        "ç« åˆ†å‰²ã‚’å®Ÿè¡Œ",
        use_container_width=True,
        disabled=not can_split_chapters,
        key="run_chapter_split",
    )
with col_step4_clear:
    clear_split_clicked = st.button(
        "ç« åˆ†å‰²çµæœã‚’ã‚¯ãƒªã‚¢",
        use_container_width=True,
        disabled=not bool(st.session_state.chapters),
        key="clear_chapter_split",
    )

if run_split_clicked:
    try:
        fixed = fix_broken_chapter_tokens(st.session_state.full_text)
        parts = split_by_chapter_linehead(fixed)
        st.session_state.chapters = parts
        st.session_state.needs_chapter_split = False
        st.success(f"æ¤œå‡ºç« æ•°: {len(parts)}")
    except Exception as e:
        st.error(f"ç« åˆ†å‰²ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

if clear_split_clicked:
    st.session_state.chapters = []
    st.session_state.needs_chapter_split = False
    st.info("ç« åˆ†å‰²çµæœã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")

if st.session_state.get("needs_chapter_split") and can_split_chapters and not st.session_state.get("chapters"):
    try:
        fixed = fix_broken_chapter_tokens(st.session_state.full_text)
        parts = split_by_chapter_linehead(fixed)
        st.session_state.chapters = parts
        st.session_state.needs_chapter_split = False
        if parts:
            st.success(f"æ¤œå‡ºç« æ•°: {len(parts)}")
        else:
            st.warning("ç« è¦‹å‡ºã—ãŒæ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å¿…è¦ã«å¿œã˜ã¦æœ¬æ–‡ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        st.error(f"ç« åˆ†å‰²ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        st.session_state.needs_chapter_split = False

if st.session_state.chapters:
    st.success(f"æ¤œå‡ºç« æ•°: {len(st.session_state.chapters)}")
    with st.expander("ç« ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆä¸Šä½3ç« ï¼‰", expanded=False):
        for title, body in st.session_state.chapters[:3]:
            st.markdown(f"### {title}")
            st.text(body[:800] + ("\nâ€¦(ä»¥ä¸‹ç•¥)" if len(body) > 800 else ""))
elif not can_split_chapters:
    st.info("Step 3 ã§ OCR ã‚’å®Ÿè¡Œã—æœ¬æ–‡ã‚’å–å¾—ã—ã¦ãã ã•ã„ã€‚")
elif not st.session_state.get("needs_chapter_split"):
    st.info("ç« è¦‹å‡ºã—ãŒè¦‹ã¤ã‹ã‚‰ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚æœ¬æ–‡ã‚’ç¢ºèªã™ã‚‹ã‹ã€ã‚«ã‚¹ã‚¿ãƒ ãƒ«ãƒ¼ãƒ«ã§å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")

# Step 5: è¦ç´„
st.subheader("Step 5. è¦ç´„ç”Ÿæˆ")
if st.session_state.chapters:
    # ã¾ã¨ã‚ã¦è¦ç´„
    if st.button("å…¨ç« ã‚’è¦ç´„ã™ã‚‹", type="primary", use_container_width=True):
        st.session_state.summaries = {}
        for idx, (title, body) in enumerate(st.session_state.chapters, start=1):
            with st.spinner(f"{idx}/{len(st.session_state.chapters)} è¦ç´„ä¸­: {title}"):
                summary = best_effort_summarize(title, body)
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é•·åˆ¶å¾¡
                if not _is_llm_available():
                    summary = summary[:st.session_state.get("max_chars", 1200)]
                st.session_state.summaries[title] = summary
        st.success("å…¨ç« ã®è¦ç´„ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    if st.session_state.summaries:
        with st.expander("è¦ç´„çµæœï¼ˆä¸Šä½3ç« ï¼‰", expanded=True):
            for i, (title, summ) in enumerate(list(st.session_state.summaries.items())[:3], start=1):
                st.markdown(f"## {title}")
                st.write(summ)

# Step 6: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
st.subheader("Step 6. ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
if st.session_state.summaries:
    st.write("Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å‡ºåŠ›ã—ã¾ã™ã€‚åˆå›ã¯ãƒ–ãƒ©ã‚¦ã‚¶ã§Googleèªè¨¼ãŒæ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚")
    default_book_title = st.session_state.get("book_title_input", "Kindleæ›¸ç±")
    default_root = st.session_state.get("drive_root_input", "OCRçµæœ")
    book_title_input = st.text_input("æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆGoogleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåã«ä½¿ç”¨ï¼‰", value=default_book_title)
    drive_root_input = st.text_input("Google Driveã®ä¿å­˜å…ˆãƒ«ãƒ¼ãƒˆãƒ•ã‚©ãƒ«ãƒ€", value=default_root)
    st.session_state.book_title_input = book_title_input
    st.session_state.drive_root_input = drive_root_input

    if st.button("Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆ", type="primary", use_container_width=True):
        if create_google_doc_external is None or get_google_credentials is None:
            st.error("google_api_utils.py ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡ºåŠ›ã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ã€‚")
        else:
            try:
                creds = st.session_state.get("google_creds")
                if creds is None:
                    with st.spinner("Googleã‚¢ã‚«ã‚¦ãƒ³ãƒˆèªè¨¼ä¸­â€¦"):
                        creds = get_google_credentials()
                    st.session_state.google_creds = creds

                chapters_for_doc = (
                    st.session_state.chapters
                    if st.session_state.chapters
                    else [("æœ¬æ–‡", st.session_state.full_text or "")]
                )
                full_content = _build_google_doc_content(chapters_for_doc)
                summary_sections = list(st.session_state.summaries.items())
                summary_content = _build_google_doc_content(summary_sections)

                with st.spinner("æ–‡ç« å…¨ä½“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆä¸­â€¦"):
                    create_google_doc_external(
                        book_title_input,
                        "æ–‡ç« å…¨ä½“",
                        full_content,
                        creds,
                        root_name=drive_root_input or "OCRçµæœ",
                    )
                with st.spinner("è¦ç´„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆä¸­â€¦"):
                    create_google_doc_external(
                        book_title_input,
                        "è¦ç´„",
                        summary_content,
                        creds,
                        root_name=drive_root_input or "OCRçµæœ",
                    )
                st.success("Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚Google Drive ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
            except Exception as e:
                st.error(f"Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

st.divider()
st.caption(
    "ğŸ’¡ GEMINI_API_KEY / GEMINI_MODEL ãŒè¨­å®šã•ã‚Œã¦ã„ã‚Œã°LLMè¦ç´„ã‚’è‡ªå‹•ã§ä½¿ç”¨ã—ã¾ã™ã€‚ç’°å¢ƒãŒæ•´ã£ã¦ã„ãªã„å ´åˆã§ã‚‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ä¸€é€šã‚Šå‹•ä½œã—ã€ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã§ã¯æ–‡ç« å…¨ä½“ã¨è¦ç´„ã®Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆã§ãã¾ã™ã€‚"
)
